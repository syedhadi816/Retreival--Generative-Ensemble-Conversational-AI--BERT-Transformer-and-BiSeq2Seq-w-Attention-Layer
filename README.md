> ## This repository is a work in progress. 

# Retreival-and-Generative-Ensemble-for-Conversational-AI--BERT-Transformer-and-Bi-Seq2Seq-with-Attention

The program begins by taking speech input from the user and converting it into text input using a speech-to-text program. This text is then processed and passed to the primary model that has been trained on the training data questions. This is a BERT dense vectorization model that is pretrained by creating embeddings of all questions within the dataset. The vector representation of the user input is obtained using the BERT transformer and is then compared with all questions within the training data. Cosine similarity is evaluated, and answers corresponding to the top k candidate questions (with the highest cosine similarity) are returned, and then fed to a seq2seq model along with the user query (k+1 inputs to seq2seq). The user query is fed to the seq2seq model to ensure that a newly generated sentence can be similar to the user query. The seq2seq model will be using an attention layer between the encoder and the decoder because it has been observed that answers within the training data be much longer than answers in most natural conversations, and this may present a challenge for relevant sentence generation. The attention layer shortens these answers by picking the prominent elements from within them and these are then passed onto the decoder that generates a new response.
After the seq2seq model, there are k answers (corresponding to the top k candidate) questions and 1 answer that is generated by the seq2seq model. A BLUE score is calculated for all k+1 responses against the query and the answer with the highest BLUE score is moved forward to the pretrained text-to-speech implementation. This is then presented to the user.

As of now, no such publicly available AI model exists for users to engage with; additionally, any such model that may exist is behind paywalls and is not completely free and open source.
