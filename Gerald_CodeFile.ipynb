{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f796b82",
   "metadata": {},
   "source": [
    "### GERALD , Conversational AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714735b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DPS\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "GERALD , Conversational AI.\n",
    "\n",
    "An open-source and open-domain conversation AI model that utilizes a sequential ensemble \n",
    "of retrieval- and generation-based systems to intelligently react to user queries while\n",
    "maintaining structural and contextual relevance.\n",
    "'''\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# importing all required libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keytotext import pipeline\n",
    "import time \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pyaudio\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import msvcrt as m\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle as pk\n",
    "\n",
    "#Initialize the objects here\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens') #BERT Sentence Transformer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\") #T5 Pretrained Tokenizer\n",
    "model_2 = T5ForConditionalGeneration.from_pretrained(\"t5-large\") #T5 Conditional Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e47bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the files.\n",
    "df = pd.read_csv('q_a_pairs.csv') #Question Answer pairs\n",
    "q_pca = np.loadtxt('q_pca_embed.txt') #Vector embeddings of questions \n",
    "pca = pk.load(open(\"pca.pkl\",'rb')) # pca model for reducing dimensions of user input\n",
    "\n",
    "# dataframe datatype fixed, and irrelevant Index columns removed\n",
    "df['Answer'] = df['Answer'].astype(str)\n",
    "df = df[['Question', 'Answer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc37e7",
   "metadata": {},
   "source": [
    "# Text Input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae204d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the author of Harry Potter\n",
      "harry potter was first published by bloomsbury in 1913.\n",
      "\n",
      " \n",
      "\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "# Input for the user to Initialize the conversation\n",
    "\n",
    "#Type 'quit' to Exit\n",
    "\n",
    "inp = input()\n",
    "\n",
    "#while quit is not in the input, the code will execute the conditions accordingly.\n",
    "while (inp!='quit'):\n",
    "    target = model.encode(inp) #user input transformed in vector of length 768 \n",
    "    target = target.reshape(1,-1) #vector representation reshaped\n",
    "    target = pca.transform(target) #pca to reduce dimensions of user input\n",
    "    match = cosine_similarity(q_pca,target) #cosine similarity of user input against dataset\n",
    "    match = np.dstack(match)[0][0] #vector of cosine similarities\n",
    "    max_index = np.argsort(match)[::-1][:3] #index of top 3 responses extracted\n",
    "    \n",
    "    \n",
    "    \n",
    "    #LAYER:1\n",
    "    # If match > 90%, use top matched response and feed to generative model  \n",
    "    if max(match) > 0.90:\n",
    "        i = max_index[0] #index of top match\n",
    "        #summarize function of t5 for a string containing data query and data response. \n",
    "        enc = 'summarize: '+ df['Question'].iloc[i] + ' ' + df['Answer'].iloc[i] #encoder\n",
    "        input_ids = tokenizer(enc, return_tensors=\"pt\").input_ids \n",
    "        #decoder,  max length cap 1.5(length of response)\n",
    "        outputs = model_2.generate(input_ids, max_length = 1.5*len(df['Answer'].iloc[i])) #decoder\n",
    "        print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #model output \n",
    "        print('\\n \\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "    #LAYER:2\n",
    "    else:\n",
    "     # else take top 3 matches and feed to generative model. \n",
    "        ret = 'summarize: '\n",
    "        for i in max_index: #loop and generate a new response from each retrieved response and query pair \n",
    "            enc = 'summarize: '+ df['Question'].iloc[i] + ' ' + df['Answer'].iloc[i] #encoder\n",
    "            input_ids = tokenizer(enc, return_tensors=\"pt\").input_ids #\n",
    "            outputs = model_2.generate(input_ids, max_length = 25) #decoder, max length cap 25\n",
    "            ret += ' ' + tokenizer.decode(outputs[0], skip_special_tokens=True) #store generated response in a string\n",
    "            \n",
    "        input_ids_2 = tokenizer(ret, return_tensors=\"pt\").input_ids\n",
    "        outputs_2 = model_2.generate(input_ids_2, max_length = 60) #decoder, max length cap 60\n",
    "        print(tokenizer.decode(outputs_2[0], skip_special_tokens=True)) #print output\n",
    "        print('\\n \\n')\n",
    "    inp = input()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f0fa7",
   "metadata": {},
   "source": [
    "### Speech to text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ac9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech to text init \n",
    "r = sr.Recognizer() \n",
    "# function to listen from the user\n",
    "def hear():\n",
    "    print('speak now')\n",
    "    with sr.Microphone() as source: \n",
    "        audio = r.listen(source)\n",
    "        text = r.recognize_google(audio) # google audio recognize here\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad75da",
   "metadata": {},
   "source": [
    "### Text to Speech function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11586c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to speech Init the voice property of the device\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id) ## for male voice\n",
    "\n",
    "# function to speck with run and wait\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d52c6",
   "metadata": {},
   "source": [
    "# Speech - Text Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082113c1",
   "metadata": {},
   "source": [
    "### Run this block of code if you wish to provide voice input and recieve audio output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for the user to Initialize the conversation\n",
    "#speak 'quit' to exit\n",
    "\n",
    "print('Press enter to begin\\n')\n",
    "input()\n",
    "# Listening fucntion defined here\n",
    "inp = hear()\n",
    "#while quit is not in the input, the code will execute the conditions accordingly.\n",
    "while (inp!='quit'):\n",
    "    print(inp)\n",
    "    target = model.encode(inp)\n",
    "    target = target.reshape(1,-1)\n",
    "    target = pca.transform(target)\n",
    "    match = cosine_similarity(q_pca,target)\n",
    "    match = np.dstack(match)[0][0]\n",
    "    max_index = np.argsort(match)[::-1][:3]\n",
    "    # If it mathces 0.90 then it will generate matched sentences \n",
    "    if max(match) > 0.90:\n",
    "        i = max_index[0]\n",
    "        #ret = 'summarize: '\n",
    "        enc = 'summarize: '+ df['Question'].iloc[i] + ' ' + df['Answer'].iloc[i]\n",
    "        input_ids = tokenizer(enc, return_tensors=\"pt\").input_ids\n",
    "        outputs = model_2.generate(input_ids, max_length = 1.5*len(df['Answer'].iloc[i]))\n",
    "        sent = (tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    else:\n",
    "     # else it will summarize and generate the relevent sentences\n",
    "        ret = 'summarize: '\n",
    "        for i in max_index:\n",
    "            enc = 'summarize: '+ df['Question'].iloc[i] + ' ' + df['Answer'].iloc[i]\n",
    "            input_ids = tokenizer(enc, return_tensors=\"pt\").input_ids\n",
    "            outputs = model_2.generate(input_ids, max_length = 25)\n",
    "            ret += ' ' + tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        input_ids_2 = tokenizer(ret, return_tensors=\"pt\").input_ids\n",
    "        outputs_2 = model_2.generate(input_ids_2, max_length = 60)\n",
    "        sent = (tokenizer.decode(outputs_2[0], skip_special_tokens=True))\n",
    "    print(sent)\n",
    "    # speak fucntion is defined here\n",
    "    speak(sent)\n",
    "    print('Press enter to speak again\\n')\n",
    "    input()\n",
    "    inp = hear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
